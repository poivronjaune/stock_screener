{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create sqlite database from CSV files  \n",
    "#### Use at our own risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use as is, you need to know what you are doing to prevent overwriting important data - you have been warned\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# Change db_name to what you need\n",
    "db_name = \"z_newdb.sqlite\"\n",
    "\n",
    "connection = sqlite3.connect(db_name)\n",
    "symbols_table = \"symbols\"\n",
    "symbols_df_from_csv = pd.read_csv(\"symbols.csv\")\n",
    "symbols_df_from_csv.drop([\"Unnamed: 0\"], axis=1, inplace=True)\n",
    "symbols_df_from_csv.to_sql(\"symbols\", connection)\n",
    "connection.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# Make sur the first run has no prices_daily table or it will append\n",
    "# Repeat for each year you want to load\n",
    "csv_file = \"prices_2014.csv\"\n",
    "\n",
    "connection = sqlite3.connect(db_name)  # db_name is initialized in previous cell\n",
    "prices_table = \"prices_daily\"\n",
    "prices_df_from_csv = pd.read_csv(csv_file)\n",
    "prices_df_from_csv.to_sql(\"prices_daily\", connection, if_exists=\"append\")\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database export to CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export SQLite3 Symbols to CSV file for github push\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "con100 = sqlite3.connect(\"TSX_Prices.sqlite\")\n",
    "sql = f\"SELECT * FROM 'symbols' ORDER BY ticker ASC\"\n",
    "data = pd.read_sql_query(sql, con100)\n",
    "data.drop(labels=\"index\", axis=1, inplace=True)\n",
    "data.to_csv(\"symbols.csv\", index=\"False\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export SQLite3 Prices to CSV file for github push using yearly export\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "year=\"2021\"\n",
    "conn = sqlite3.connect(\"TSX_Prices.sqlite\")\n",
    "sql = f\"SELECT * FROM 'prices_daily' WHERE Date LIKE '{year}%' ORDER BY ticker ASC\"\n",
    "data = pd.read_sql_query(sql, conn)\n",
    "data.drop(labels=\"index\", axis=1, inplace=True)\n",
    "data[\"Date\"] = pd.to_datetime(data[\"Date\"], infer_datetime_format=True)\n",
    "data[\"Date\"] = data[\"Date\"].dt.date\n",
    "data.to_csv(f\"prices_{year}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 587337 entries, 0 to 587336\n",
      "Data columns (total 7 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   Date    587337 non-null  object \n",
      " 1   Ticker  587337 non-null  object \n",
      " 2   Open    587337 non-null  object \n",
      " 3   High    587337 non-null  object \n",
      " 4   Low     587337 non-null  object \n",
      " 5   Close   587337 non-null  float64\n",
      " 6   Volume  587337 non-null  float64\n",
      "dtypes: float64(2), object(5)\n",
      "memory usage: 31.4+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WEB Scraping functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chromedriver : Open a chrome browser to use for scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open a Chrome Browser that will be controlled by Selemium\n",
    "# No page loaded in driver\n",
    "import time\n",
    "import warnings\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "# Setup Selenium browser\n",
    "CHROME_DRIVER_LOCATION = \"chromedriver.exe\"\n",
    "OPTIONS = webdriver.ChromeOptions()\n",
    "OPTIONS.add_argument('--ignore-certicate-errors')\n",
    "OPTIONS.add_argument('--incognito')\n",
    "#OPTIONS.add_argument('--headless')\n",
    "OPTIONS.add_experimental_option('excludeSwitches', ['enable-logging'])\n",
    "driver = webdriver.Chrome(executable_path=CHROME_DRIVER_LOCATION,options=OPTIONS)\n",
    "#driver.implicitly_wait(10)\n",
    "wait = WebDriverWait(driver, 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to open the TSX listings page and selected a letter and exchange to extract data from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to open TSX Page for symbols listed, select the exchange and grab data from page\n",
    "# Complicated scraper as data is JAVASCRIPT generated, cannot simply use pandas_datareader\n",
    "def tickers_currenlty_listed(letter, exchange):\n",
    "    tsx_url = f\"https://www.tsx.com/listings/listing-with-us/listed-company-directory\"\n",
    "    driver.get(tsx_url)\n",
    "    \n",
    "    # Set exchange for page\n",
    "    try:\n",
    "        btn_switch = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.ID, 'exchange-toggle')))\n",
    "        if exchange == \"tsx\" and btn_switch.get_attribute(\"class\") == \"btn-switch invert\":\n",
    "            btn_switch.click()\n",
    "        if exchange == \"tsxv\" and btn_switch.get_attribute(\"class\") == \"btn-switch\":\n",
    "            btn_switch.click()\n",
    "    except Exception as e:\n",
    "        print(f\"Unable to locate exchange-toggle button, \",e)\n",
    "        return {}\n",
    "\n",
    "    # Push letter+ENTER in search field\n",
    "    try:\n",
    "        search = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.ID, 'query')))\n",
    "        search.send_keys(letter + Keys.ENTER)\n",
    "    except Exception as e:\n",
    "        print(f\"Unable to send_keys in inut field [{letter} ], \",e)\n",
    "        return {}    \n",
    "\n",
    "    # Extract data using selenium ad create a list\n",
    "    try:\n",
    "        #WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH, \"//*[text()='Symbol']\")))\n",
    "        result_data_xpath = '//*[@id=\"tresults\"]/tbody'\n",
    "        WebDriverWait(driver, 5).until( EC.visibility_of_element_located((By.XPATH, result_data_xpath)))\n",
    "        datagrid = driver.find_element_by_xpath(result_data_xpath)\n",
    "    except Exception as e:\n",
    "        print(f\"Unable to locate HTML Table containing listing data, \",e)\n",
    "        return {}    \n",
    "\n",
    "    data_rows = datagrid.find_elements(By.TAG_NAME, \"tr\")\n",
    "    data = []\n",
    "    for row in data_rows:\n",
    "        cells = row.find_elements(By.TAG_NAME, \"td\")\n",
    "        # Skip the line with no ticker symbol\n",
    "        if (cells[1].text != \"\"):\n",
    "            ticker = cells[1].text\n",
    "            name = cells[0].text\n",
    "            url = f\"https://money.tmx.com/en/quote/{ticker}\"\n",
    "            row_info = {\"ticker\": ticker, \"company\":name.strip(), \"exchange\": exchange, \"url\":url, \"yahoo\":\"-\"}\n",
    "            data.append(row_info)\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract symbol's list for tickers or companies startign with P on the tsx echanges\n",
    "# use \"tsx\" or \"tsxv\" for second parameter\n",
    "data200 = tickers_currenlty_listed(\"P\", \"tsx\")\n",
    "data200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to open the TSX Trade history page for a specified TICKER symbol and extract data to a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract trading history for tmx web site and store in a CSV file\n",
    "# Parameters:\n",
    "# - symbol to extract\n",
    "# - pages_to_read : number of historical pages to read (selenium script will click on the navigation buttons automatically)\n",
    "# - msg1 : Use when looping through a bunch of tickers to follow progress\n",
    "def extract_trading_history(symbol, pages_to_read, msg1):\n",
    "    tmx_url = f\"https://money.tmx.com/en/quote/{symbol}/trade-history?selectedTab=price-history\"\n",
    "    driver.get(tmx_url)\n",
    "    header_flag = True\n",
    "    #username = input(\"Please click in web page then ENTER\")\n",
    "    time.sleep(5)\n",
    "\n",
    "    # CLOSE THE Freaking AD at bottom of screen to expose the next button\n",
    "    ad_closed = False\n",
    "    try:\n",
    "        #close_ad = WebDriverWait(driver, 5).until(presence_of_element_located(By.ID('ssrt-close-anchor-button')))\n",
    "        close_ad = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.ID, 'ssrt-close-anchor-button')))\n",
    "        print(type(close_ad))\n",
    "        close_ad.click()\n",
    "        ad_closed = True\n",
    "    except Exception as e:\n",
    "        print(f\"Unable to clase_ads : {e}\")   \n",
    "\n",
    "    if ad_closed:\n",
    "        # Set to pages_to_read to 45 for a good 2 years back data ( for does not loop through last number of range)\n",
    "        fin_loop = pages_to_read + 1\n",
    "        prev_date = \"None\"\n",
    "        for i in range(1,fin_loop):\n",
    "            html_page = driver.page_source\n",
    "            try:\n",
    "                data = pd.read_html(html_page)\n",
    "                prices_df = data[0]\n",
    "                #print(f\"Data extracted: {i}, date at bottom : {prices_df['Date'].iloc[-1]}\")\n",
    "                print(f\"{msg1} => {symbol} [{i}/{fin_loop - 1}] :Extracted date: {prices_df['Date'].iloc[-1]}, Previous Date Extracted: {prev_date}\")\n",
    "                filename = f\"CSV\\{symbol}.csv\"\n",
    "                prices_df.to_csv(filename, mode='a', header=header_flag)\n",
    "                header_flag = False\n",
    "                prev_date = prices_df['Date'].iloc[-1]        \n",
    "                # Find the next button and click it\n",
    "                btn_next = driver.find_element(By.XPATH, \"//button[@data-testid='next-button']\")\n",
    "                btn_next.click() \n",
    "            except:\n",
    "                i = fin_loop\n",
    "    \n",
    "    print(\"End of extraction\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_trading_history(\"A\", 2, \"No msg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data300 = pd.read_csv(\"CSV/A.csv\")\n",
    "data300['Date'] = pd.to_datetime(data300[\"Date\"], infer_datetime_format=True)\n",
    "data300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to save a DataFrame (loaded from CSV file) to database table : prices_daily  \n",
    "Date from CSV must be formatted to yyyy-mm-dd  \n",
    "Data will be appended only for dates greater than existing Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "def update_prices_daily(ticker, price_data):\n",
    "        conn = sqlite3.connect(\"TSX_Prices.sqlite\")\n",
    "        if conn is not None:\n",
    "            symbol_to_load = ticker\n",
    "            existing_prices_df = pd.read_sql(f\"SELECT * FROM 'prices_daily' WHERE Ticker='{symbol_to_load}' ORDER BY Date DESC LIMIT 1\", conn)\n",
    "        try:\n",
    "            # Only keep dataframe dates that are greater than the existing ones in prices_daily\n",
    "            last_date = existing_prices_df.loc[0][\"Date\"]\n",
    "            new_prices_df = price_data.loc[price_data[\"Date\"] > last_date]\n",
    "        except KeyError:\n",
    "            last_date = None\n",
    "            new_prices_df = price_data\n",
    "\n",
    "        # Structure the data according to database table structure\n",
    "        new_prices_df.drop(['VWAP ($)', 'Change ($)', 'Trade Value', '# Trades', 'Change (%)'], axis=1, inplace=True)\n",
    "        new_prices_df.rename(columns={'Open ($)': 'Open', 'High ($)': 'High', 'Low ($)': 'Low', 'Close ($)': 'Close'  }, inplace=True)\n",
    "        new_prices_df[\"Ticker\"] = symbol_to_load\n",
    "        #new_prices_df['Date'] = pd.to_datetime(data_cleaned[\"Date\"], infer_datetime_format=True)\n",
    "        new_prices_df = new_prices_df.reindex([\"Date\",\"Ticker\",\"Open\",\"High\",\"Low\",\"Close\",\"Volume\"],axis=1)\n",
    "        # Drop duplicate dates\n",
    "        new_prices_df.drop_duplicates(subset=\"Date\", inplace=True)\n",
    "        new_prices_df.to_sql(\"prices_daily\", conn, if_exists=\"append\")\n",
    "\n",
    "        print(f\"\\n\\nUpdating 'prices_daily': {ticker} \\n{price_data} \\nExisting Prices\\n{existing_prices_df} \\nLast date: {last_date} \\nNew Prices:\\n{new_prices_df}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_prices_daily(\"A\", data300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLD STUFF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Tools to Extract and Build a Historical Data Base of TSX Daily Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilty function to load CSV files scraped from TSX Trading history web page into our new Data Base\n",
    "def add_csv_data_to_daily_prices(symbol_to_load):\n",
    "    # Load CSV file of Daily Data scraped from TMX web site\n",
    "    # Clean the data make sure all rows are unique\n",
    "    # Structure the data according to database table structure\n",
    "    # Load existing data from daily_prices for symbol\n",
    "    # Merge cleaned data to existing data\n",
    "    # Wipe all duplicate dates\n",
    "    # if new data is available, append to existing daily_prices\n",
    "\n",
    "\n",
    "    # Load a CSV of Daily Price Data and make sure no duplicates exist\n",
    "    #symbol_to_load = \"new\"\n",
    "    filename = f\"{symbol_to_load}.csv\"\n",
    "    data = pd.read_csv(f\"CSV\\{filename}\")\n",
    "    # Clean the data make sure all new data rows are unique\n",
    "    if data.Date.is_unique == False:\n",
    "        data_cleaned = data[~data.Date.duplicated()]\n",
    "    else:\n",
    "        data_cleaned = data\n",
    "    #print(f\"({data.shape[0]}, {data_cleaned.shape[0]})\")\n",
    "\n",
    "    # Structure the data according to database table structure\n",
    "    data_cleaned.drop(['Unnamed: 0', 'VWAP ($)', 'Change ($)', 'Trade Value', '# Trades', 'Change (%)'], axis=1, inplace=True)\n",
    "    data_cleaned.rename(columns={'Open ($)': 'Open', 'High ($)': 'High', 'Low ($)': 'Low', 'Close ($)': 'Close'  }, inplace=True)\n",
    "    data_cleaned[\"Ticker\"] = symbol_to_load\n",
    "    data_cleaned['Date'] = pd.to_datetime(data_cleaned[\"Date\"], infer_datetime_format=True)\n",
    "    data_cleaned = data_cleaned.reindex([\"Date\",\"Ticker\",\"Open\",\"High\",\"Low\",\"Close\",\"Volume\"],axis=1)\n",
    "\n",
    "    # Load existing data from daily_prices for symbol\n",
    "    database = \"TSX_Prices.sqlite\"\n",
    "    con3 = sqlite3.connect(database)\n",
    "    existing_prices_df = pd.read_sql(f\"SELECT * FROM 'prices_daily' WHERE Ticker='{symbol_to_load}'\", con3)\n",
    "\n",
    "    # Keep data_cleaned prices that are not in existing_prices (using Date column)\n",
    "    #data_to_add = data_cleaned[~(data_cleaned['Date'].isin(existing_prices_df['Date']))].reset_index(drop=True)\n",
    "    data_to_add = data_cleaned[~(data_cleaned['Date'].isin(existing_prices_df['Date']))].reset_index(drop=True)\n",
    "    # Append Data to database\n",
    "    data_to_add.to_sql(\"prices_daily\", con3, if_exists=\"append\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trade history data reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all tickers and ompany names and count the number price_rows available for each ticker, inlcuding star and end dates for each sorted by decreasing count of price_rows\n",
    "con3 = sqlite3.connect(\"TSX_Prices.sqlite\")\n",
    "cursor2 = con3.cursor()\n",
    "sql3 = \"SELECT DISTINCT prices_daily.Ticker, symbols.name, min(prices_daily.Date) as start_date, max(prices_daily.Date) as end_date, count(prices_daily.Ticker) as price_rows FROM prices_daily INNER JOIN symbols ON prices_daily.Ticker = symbols.ticker GROUP BY prices_daily.ticker ORDER BY price_rows\"\n",
    "prices_data1 = pd.read_sql_query(sql3, con3)\n",
    "prices_data1['start_date'] = pd.to_datetime(prices_data1['start_date']).dt.normalize()\n",
    "prices_data1['end_date'] = pd.to_datetime(prices_data1['end_date']).dt.normalize()\n",
    "prices_data1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORT AND TEST Stocksdata.py functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stocksdata import *\n",
    "stocks_data = StocksData(db=\".\\TSX_Prices.sqlite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = stocks_data.count_prices_per_ticker(count=True)\n",
    "df1.sort_values('price_rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks_data.update_symbol(\"ZZZ999\",\"Some invalid company\",\"tsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering Pandas Dataframe on Ticker starts with \"AB*\"\n",
    "filter = \"AB\"\n",
    "df_copy = df1[df1[\"ticker\"].str[0:len(filter)] == filter].copy(deep=True)\n",
    "\n",
    "#symbols_df.loc[symbols_df[\"ticker\"].str[0:end_pos] == ticker_filter].copy(deep=True)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d67b9618866dd9b74d33a444efc9c9bbb974433ef649364e533196a1f341c63f"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
