{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Tools to Extract and Build a Historical Data Base of TSX Daily Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load required python packages\n",
    "# https://www.selenium.dev/selenium/docs/api/py/api.html\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from lxml import etree\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to yahoo populated price database that had one table per ticker symbol (not practical)\n",
    "# Merge all yahoo extracted data to the new TSX_Prices.sqlite database for futur use\n",
    "con1 = sqlite3.connect(\"TSX_Data.sqlite\")\n",
    "table = \"tsx_symbols\"\n",
    "sql_request = f\"SELECT * FROM '{table}'\"\n",
    "symbols_df = pd.read_sql_query(sql_request, con1)\n",
    "symbols_df.drop(['level_0','index'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the new Daily Prices Database\n",
    "# Transfer the scraped ticker symbols from TSX to thi snew database  (uncomment for first run)\n",
    "con2 = sqlite3.connect(\"TSX_Prices.sqlite\")\n",
    "#symbols_df.to_sql(\"symbols\", con2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DON'T RUN THIS FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT RUN BY DEFAULT, \n",
    "# ONLY USE if you deleted TSX_Prices.sqlite\n",
    "# Merge all individual tables with prices to a common daily price table and add the ticker symbol to each day of data\n",
    "for symbol, ticker in zip(symbols_df[\"yahoo\"],symbols_df[\"ticker\"]):\n",
    "    table_name = symbol\n",
    "    sql1 = f\"SELECT * FROM '{table_name}'\"\n",
    "    prices_df = pd.read_sql_query(sql1, con1)\n",
    "    prices_df.drop(['index'], axis=1, inplace=True)\n",
    "    if not prices_df[\"Volume\"].isnull().any():\n",
    "        prices_df[\"Ticker\"] = ticker\n",
    "        prices_df = prices_df.reindex([\"Date\",\"Ticker\",\"Open\",\"High\",\"Low\",\"Close\",\"Volume\"],axis=1)\n",
    "        prices_df.to_sql(\"prices_daily\", con2, if_exists=\"append\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilty function to load CSV files scraped from TSX Trading history web page into our new Data Base\n",
    "def add_csv_data_to_daily_prices(symbol_to_load):\n",
    "    # Load CSV file of Daily Data scraped from TMX web site\n",
    "    # Clean the data make sure all rows are unique\n",
    "    # Structure the data according to database table structure\n",
    "    # Load existing data from daily_prices for symbol\n",
    "    # Merge cleaned data to existing data\n",
    "    # Wipe all duplicate dates\n",
    "    # if new data is available, append to existing daily_prices\n",
    "\n",
    "\n",
    "    # Load a CSV of Daily Price Data and make sure no duplicates exist\n",
    "    #symbol_to_load = \"new\"\n",
    "    filename = f\"{symbol_to_load}.csv\"\n",
    "    data = pd.read_csv(f\"CSV\\{filename}\")\n",
    "    # Clean the data make sure all new data rows are unique\n",
    "    if data.Date.is_unique == False:\n",
    "        data_cleaned = data[~data.Date.duplicated()]\n",
    "    else:\n",
    "        data_cleaned = data\n",
    "    #print(f\"({data.shape[0]}, {data_cleaned.shape[0]})\")\n",
    "\n",
    "    # Structure the data according to database table structure\n",
    "    data_cleaned.drop(['Unnamed: 0', 'VWAP ($)', 'Change ($)', 'Trade Value', '# Trades', 'Change (%)'], axis=1, inplace=True)\n",
    "    data_cleaned.rename(columns={'Open ($)': 'Open', 'High ($)': 'High', 'Low ($)': 'Low', 'Close ($)': 'Close'  }, inplace=True)\n",
    "    data_cleaned[\"Ticker\"] = symbol_to_load\n",
    "    data_cleaned['Date'] = pd.to_datetime(data_cleaned[\"Date\"], infer_datetime_format=True)\n",
    "    data_cleaned = data_cleaned.reindex([\"Date\",\"Ticker\",\"Open\",\"High\",\"Low\",\"Close\",\"Volume\"],axis=1)\n",
    "\n",
    "    # Load existing data from daily_prices for symbol\n",
    "    database = \"TSX_Prices.sqlite\"\n",
    "    con3 = sqlite3.connect(database)\n",
    "    existing_prices_df = pd.read_sql(f\"SELECT * FROM 'prices_daily' WHERE Ticker='{symbol_to_load}'\", con3)\n",
    "\n",
    "    # Keep data_cleaned prices that are not in existing_prices (using Date column)\n",
    "    #data_to_add = data_cleaned[~(data_cleaned['Date'].isin(existing_prices_df['Date']))].reset_index(drop=True)\n",
    "    data_to_add = data_cleaned[~(data_cleaned['Date'].isin(existing_prices_df['Date']))].reset_index(drop=True)\n",
    "    # Append Data to database\n",
    "    data_to_add.to_sql(\"prices_daily\", con3, if_exists=\"append\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trade history data reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Selenium browser\n",
    "CHROME_DRIVER_LOCATION = \"chromedriver.exe\"\n",
    "OPTIONS = webdriver.ChromeOptions()\n",
    "OPTIONS.add_argument('--ignore-certicate-errors')\n",
    "OPTIONS.add_argument('--incognito')\n",
    "#OPTIONS.add_argument('--headless')\n",
    "OPTIONS.add_experimental_option('excludeSwitches', ['enable-logging'])\n",
    "driver = webdriver.Chrome(executable_path=CHROME_DRIVER_LOCATION,options=OPTIONS)\n",
    "#driver.implicitly_wait(10)\n",
    "wait = WebDriverWait(driver, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_trading_history(symbol, pages_to_read, msg1):\n",
    "    tmx_url = f\"https://money.tmx.com/en/quote/{symbol}/trade-history?selectedTab=price-history\"\n",
    "    driver.get(tmx_url)\n",
    "    header_flag = True\n",
    "    #username = input(\"Please click in web page then ENTER\")\n",
    "    time.sleep(5)\n",
    "\n",
    "    # CLOSE THE Freaking AD at bottom of screen to expose the next button\n",
    "    ad_closed = False\n",
    "    try:\n",
    "        #close_ad = WebDriverWait(driver, 5).until(presence_of_element_located(By.ID('ssrt-close-anchor-button')))\n",
    "        close_ad = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.ID, 'ssrt-close-anchor-button')))\n",
    "        print(type(close_ad))\n",
    "        close_ad.click()\n",
    "        ad_closed = True\n",
    "    except Exception as e:\n",
    "        print(f\"Unable to clase_ads : {e}\")   \n",
    "\n",
    "    if ad_closed:\n",
    "        # Set to 45 for a good 2 years back data\n",
    "        fin_loop = pages_to_read\n",
    "        prev_date = \"None\"\n",
    "        for i in range(1,fin_loop):\n",
    "            html_page = driver.page_source\n",
    "            try:\n",
    "                data = pd.read_html(html_page)\n",
    "                prices_df = data[0]\n",
    "                #print(f\"Data extracted: {i}, date at bottom : {prices_df['Date'].iloc[-1]}\")\n",
    "                print(f\"{msg1} => {symbol} [{i}/{fin_loop}] :Extracted date: {prices_df['Date'].iloc[-1]}, Previous Date Extracted: {prev_date}\")\n",
    "                filename = f\"CSV\\{symbol}.csv\"\n",
    "                prices_df.to_csv(filename, mode='a', header=header_flag)\n",
    "                header_flag = False\n",
    "                prev_date = prices_df['Date'].iloc[-1]        \n",
    "                # Find the next button and click it\n",
    "                btn_next = driver.find_element(By.XPATH, \"//button[@data-testid='next-button']\")\n",
    "                btn_next.click() \n",
    "            except:\n",
    "                i = fin_loop\n",
    "    \n",
    "    print(\"End of extraction\")            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a bunch of ticker symbols that don't have any data\n",
    "letter = \"A\" \n",
    "some_symbols_df = pd.read_sql_query(f\"SELECT * FROM 'symbols' WHERE ticker LIKE '{letter}%' ORDER BY ticker ASC\", con2)\n",
    "some_symbols_df\n",
    "tickers_to_scrap = []\n",
    "for symbol_to_analyse, yahoo in zip(some_symbols_df['ticker'],some_symbols_df['yahoo']):\n",
    "    data_df = pd.read_sql_query(f\"SELECT * FROM '{yahoo}'\", con1)\n",
    "    no_data_found = data_df['Volume'].isnull().any()\n",
    "    if no_data_found:\n",
    "        tickers_to_scrap.append(symbol_to_analyse)\n",
    "print(len(tickers_to_scrap)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str2 = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET all tickers from prices_daily that we want to update\n",
    "sym4 = \"Z\"\n",
    "con4 = sqlite3.connect(\"TSX_Prices.sqlite\")\n",
    "cursor4 = con4.cursor()\n",
    "#sql4 = \"SELECT Ticker, min(Date) as start_date, max(Date) as end_date, count(Ticker) as price_rows FROM prices_daily WHERE ticker LIKE 'A%' ORDER BY ticker ASC\"\n",
    "sql4 = f\"SELECT Ticker FROM prices_daily WHERE ticker LIKE '{sym4}%' GROUP BY Ticker ORDER BY ticker ASC\"\n",
    "prices_data4 = pd.read_sql_query(sql4, con4)\n",
    "tickers_to_scrap =prices_data4[\"Ticker\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str2 = \"\"\n",
    "for sym1 in tickers_to_scrap:\n",
    "    str2 += f\"'{sym1}',\"\n",
    "print(str2[:-1])\n",
    "print(len(tickers_to_scrap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually launch a daily price extraction\n",
    "#tickers = ['QAH','QBB','QBR.A','QBR.B','QBTC','QBTC.U','QBTL','QCD','QCE','QCLN','QCN','QDX','QDXB','QDXH','QEBH','QEBL','QEC','QEM','QETH.U','QETH.UN','QHY','QIE','QINF','QMA','QMY','QQC','QQC.F','QQEQ','QQEQ.F','QQJR','QQJR.F','QRET','QSB','QSP.UN','QSR','QTRH','QUB','QUIG','QUS','QUU','QUU.U','QXM','R.P','RA.UN','RAK','RATE','RAV.UN','RAY.A','RAY.B','RBA','RBDI','RBN.UN','RBNK','RBO','RBOT','RBOT.U','RBX','RBZ.P','RCD','RCE','RCG','RCG.PR.B','RCH','RCI.A','RCI.B','RCK','RCO.UN','RCR.P','RCT','RDG','RDL','RDS','RDU','RE','REAL','REAX','REBL.P','RECO','RECO.WT.A','RECP','REG','REI.UN','REIT','REKO','RENT.P','RET','RET.A','REVO','REX','RFC','RFP','RG','RGD','RGI','RHC','RHC.WT','RHT','RIB.UN','RID','RID.U','RIDH','RIDR.P','RIE','RIE.U','RIEH','RIFI','RIGU','RIIN','RIO','RIRA','RIT','RIWI','RJX.A','RK','RKR','RKV','RLB','RLP','RLP.DB.B','RLT.P','RLV','RLYG','RMD','RMI','RML','RMO','RMS.P','RNP','RNW','ROCK','ROI','ROK','ROMJ','ROMJ.WT','ROOF','ROOF.WT','ROOT','ROS','ROVR','ROX','RP','RPC','RPD','RPD.U','RPDH','RPF','RPI.UN','RPP','RPSB','RPX','RQI','RQJ','RQK','RQL','RQN','RQO','RQP','RR','RRI','RRR.UN','RRS','RS','RS.PR.A','RSE.P','RSI','RSI.DB.E','RSI.DB.F','RSLV','RSS','RSV','RTG','RTH','RTI','RTM','RUBH','RUBY','RUBY.U','RUD','RUD.U','RUDH','RUE','RUE.U','RUEH','RUG','RUM','RUP','RUS','RUSB','RUSB.U','RVG','RVX','RVX.WT.A','RW','RWC','RWE','RWE.B','RWU','RWU.B','RWW','RWW.B','RWX','RWX.B','RX','RXD','RXD.U','RXE','RXE.U','RY','RY.PR.H','RY.PR.J','RY.PR.M','RY.PR.N','RY.PR.O','RY.PR.P','RY.PR.S','RY.PR.Z','RYE','RYO','RYR','RYU','RZE','RZZ','S','SAAS.P','SAE','SAG','SAH','SAM','SAO','SAP','SARG.P','SAT','SAU','SAWC.P','SAY','SB','SBB','SBBC','SBC','SBC.PR.A','SBI','SBM','SBN','SBN.PR.A','SBR','SBT','SBT.B','SBT.U','SCAN','SCD','SCL','SCLT','SCOT','SCPO.UN','SCPT.A','SCPT.U','SCT','SCY','SCZ','SDC','SDE','SDI','SDR','SEA','SEB','SEC','SEI','SES','SEV','SFC','SFC.WT','SFD','SFI','SFR','SFT','SFTC','SFX','SGA','SGC','SGE','SGI','SGMD','SGN','SGO','SGQ','SGR.U','SGR.UN','SGU','SGY','SGY.DB','SGY.DB.A','SGZ','SHL','SHLE','SHOP','SHRP','SIA','SIC','SID','SIE','SIH.UN','SII','SIL','SILV','SIQ','SIS','SIXT','SJ','SJL','SJR.A','SJR.B','SKE','SKK','SKP','SKRR','SKYG','SKYY','SLF','SLF.PR.A','SLF.PR.B','SLF.PR.C','SLF.PR.D','SLF.PR.E','SLF.PR.G','SLF.PR.H','SLF.PR.I','SLF.PR.J','SLF.PR.K','SLG','SLHG','SLI','SLMN','SLR','SLS','SLVR','SM','SMA','SMAR.P','SMC','SMD','SME','SML','SMN','SMP','SMR','SMT','SMU.UN','SMY','SN','SNC','SNF','SNG','SNI.PR.A','SNM','SNS','SNV','SOC','SOCK','SOI','SOIL','SOIL.WT','SOLG','SOLR','SOMA','SOO.P','SOT.DB','SOT.UN','SOU','SOY','SPA','SPB','SPC','SPD','SPG','SPG.WT','SPI','SPN','SPOT','SPP','SPPP','SPPP.U','SPS.A','SPX','SQD','SQG','SR','SRA','SRC','SRE','SRES','SRG','SRI','SRL','SRU.UN','SRV.UN','SRX','SSA','SSE','SSF.UN','SSL','SSRM','SSS.P','SSSS.P','SSV','SSVR','SSX.P','STA','STAK.P','STC','STCK','STCK.WT','STE','STEP','STG','STGO','STH','STLC','STMP','STN','STND','STNG','STPL','STRR','STRR.WT','STS','STU','STUD','STUV','SU','SUGR','SUGR.DB','SUGR.WR','SUGR.WS','SUGR.WT','SUI','SUP','SURG','SVA','SVB','SVE','SVG','SVI','SVI.DB','SVI.DB.B','SVM','SVR','SVR.C','SVS','SVTN','SW','SWA','SWLF','SWP','SXI','SXL','SXP','SYH','SYLD','SYZ','SZLS','SZLS.WS','SZLS.WT','SZM']\n",
    "tickers = tickers_to_scrap\n",
    "pages = 2 # NUmber of TMX Trade history to scrap back\n",
    "i = 1\n",
    "total = len(tickers)\n",
    "for ticker_to_extract in tickers:\n",
    "    #ticker_to_extract = \"AVNT\"\n",
    "    try:\n",
    "        msg1 = f\"[{i}/{total} : {ticker_to_extract}]\"\n",
    "        extract_trading_history(ticker_to_extract, pages, msg1)\n",
    "        add_csv_data_to_daily_prices(ticker_to_extract)\n",
    "    except:\n",
    "        pass\n",
    "    i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con5 = sqlite3.connect(\"TSX_Prices.sqlite\")\n",
    "cursor5 = con5.cursor()\n",
    "sql5 = \"SELECT ticker FROM symbols ORDER BY ticker ASC\"\n",
    "prices_data5 = pd.read_sql_query(sql5, con5)\n",
    "prices_data5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con4 = sqlite3.connect(\"TSX_Prices.sqlite\")\n",
    "cursor4 = con4.cursor()\n",
    "#sql4 = \"SELECT Ticker, min(Date) as start_date, max(Date) as end_date, count(Ticker) as price_rows FROM prices_daily WHERE ticker LIKE 'A%' ORDER BY ticker ASC\"\n",
    "#sql4 = \"SELECT Ticker, count(Ticker), min(Date) as start_date, max(Date) as end_date FROM prices_daily WHERE ticker LIKE 'A%' GROUP BY Ticker ORDER BY ticker ASC\"\n",
    "sql4 = \"SELECT Ticker, count(Ticker) FROM prices_daily GROUP BY Ticker ORDER BY Ticker ASC\"\n",
    "prices_data4 = pd.read_sql_query(sql4, con4)\n",
    "prices_data4[0:150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prices statistics\n",
    "cursor = con2.cursor()\n",
    "# cursor.execute(\"SELECT count(*) FROM 'prices_daily'\")                                             # Count all rows of data in prices_daily\n",
    "# cursor.execute(\"SELECT DISTINCT Ticker, count() FROM 'prices_daily' GROUP BY Ticker\")             # Count rows of prices for all distinct tickers from prices_daily\n",
    "cursor.execute(\"SELECT COUNT (DISTINCT Ticker) FROM 'prices_daily'\")                                # Count unique ticker symbols with prices from prices_daily\n",
    "cursor.fetchone()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#self.full_path = os.path.abspath(self.db)\n",
    "#drivepath, self.filename = os.path.split(self.db)\n",
    "import os.path\n",
    "n1 = \"TSX_Prices.sqlite\"\n",
    "full_path = os.path.abspath(n1)\n",
    "drivepath, f1 = os.path.split(full_path)\n",
    "drive, p1 = os.path.splitdrive(drivepath)\n",
    "(drivepath, f1, drive, p1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all tickers and ompany names and count the number price_rows available for each ticker, inlcuding star and end dates for each sorted by decreasing count of price_rows\n",
    "con3 = sqlite3.connect(\"TSX_Prices.sqlite\")\n",
    "cursor2 = con3.cursor()\n",
    "sql3 = \"SELECT DISTINCT prices_daily.Ticker, symbols.name, min(prices_daily.Date) as start_date, max(prices_daily.Date) as end_date, count(prices_daily.Ticker) as price_rows FROM prices_daily INNER JOIN symbols ON prices_daily.Ticker = symbols.ticker GROUP BY prices_daily.ticker ORDER BY price_rows\"\n",
    "prices_data1 = pd.read_sql_query(sql3, con3)\n",
    "prices_data1['start_date'] = pd.to_datetime(prices_data1['start_date']).dt.normalize()\n",
    "prices_data1['end_date'] = pd.to_datetime(prices_data1['end_date']).dt.normalize()\n",
    "prices_data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.date(2021, 12, 1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime as datetime\n",
    "datetime.date.today()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import time\n",
    "con6 = sqlite3.connect(\"TSX_Prices.sqlite\")\n",
    "cursor6 = con6.cursor()\n",
    "symbol_to_load = \"BBQW\"\n",
    "sql6 = f\"SELECT * FROM 'prices_daily' WHERE Ticker='{symbol_to_load}' ORDER BY Date DESC LIMIT 1\"\n",
    "prices_data6 = pd.read_sql_query(sql6, con6)\n",
    "prices_data6\n",
    "try:\n",
    "    last_date = prices_data6.loc[0][\"Date\"]\n",
    "except KeyError as e:\n",
    "    last_date = None\n",
    "print(last_date)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Technical Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prices_data(ticker):\n",
    "    con7 = sqlite3.connect(\"TSX_Prices.sqlite\")\n",
    "    cursor7 = con7.cursor()\n",
    "    sql7 = f\"SELECT * FROM 'prices_daily' WHERE Ticker='{ticker}' ORDER BY Date DESC\"\n",
    "    prices_data7 = pd.read_sql_query(sql7, con7)\n",
    "    prices_data7.drop(labels=\"index\", axis=1, inplace=True)\n",
    "    con7.close()\n",
    "    return prices_data7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export SQLite3 Symbols to CSV file for github push\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect(\"TSX_Prices.sqlite\")\n",
    "sql = f\"SELECT * FROM 'symbols' ORDER BY ticker ASC\"\n",
    "data = pd.read_sql_query(sql, conn)\n",
    "data.drop(labels=\"index\", axis=1, inplace=True)\n",
    "data.to_csv(\"symbols.csv\", index=\"False\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sqlite3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df_7 = get_prices_data(\"ABCT\")\n",
    "data_df_7"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d67b9618866dd9b74d33a444efc9c9bbb974433ef649364e533196a1f341c63f"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
