{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create sqlite database from CSV files  \n",
    "#### Use at your own risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use as is, you need to know what you are doing to prevent overwriting important data - you have been warned\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# Change db_name to what you need\n",
    "db_name = \"z_newdb.sqlite\"\n",
    "\n",
    "connection = sqlite3.connect(db_name)\n",
    "symbols_table = \"symbols\"\n",
    "symbols_df_from_csv = pd.read_csv(\"symbols.csv\")\n",
    "symbols_df_from_csv.drop([\"Unnamed: 0\"], axis=1, inplace=True)\n",
    "symbols_df_from_csv.to_sql(\"symbols\", connection)\n",
    "connection.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# Make sur the first run has no prices_daily table or it will append\n",
    "# Repeat for each year you want to load\n",
    "csv_file = \"prices_2014.csv\"\n",
    "\n",
    "connection = sqlite3.connect(db_name)  # db_name is initialized in previous cell\n",
    "prices_table = \"prices_daily\"\n",
    "prices_df_from_csv = pd.read_csv(csv_file)\n",
    "prices_df_from_csv.to_sql(\"prices_daily\", connection, if_exists=\"append\")\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database export to CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export SQLite3 Symbols to CSV file for github push\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "con100 = sqlite3.connect(\"TSX_Prices.sqlite\")\n",
    "sql = f\"SELECT * FROM 'symbols' ORDER BY ticker ASC\"\n",
    "data = pd.read_sql_query(sql, con100)\n",
    "data.drop(labels=\"index\", axis=1, inplace=True)\n",
    "data.to_csv(\"symbols.csv\", index=\"False\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export SQLite3 Prices to CSV file for github push using yearly export\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "year=\"2013\"\n",
    "conn = sqlite3.connect(\"TSX_Prices.sqlite\")\n",
    "sql = f\"SELECT * FROM 'prices_daily' WHERE Date LIKE '{year}%' ORDER BY ticker ASC, Date DESC\"\n",
    "data = pd.read_sql_query(sql, conn)\n",
    "data.drop(labels=\"index\", axis=1, inplace=True)\n",
    "data[\"Date\"] = pd.to_datetime(data[\"Date\"], infer_datetime_format=True)\n",
    "data[\"Date\"] = data[\"Date\"].dt.date\n",
    "data.to_csv(f\"prices_{year}.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WEB Scraping functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chromedriver : Open a chrome browser to use for scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open a Chrome Browser that will be controlled by Selemium\n",
    "# No page loaded in driver\n",
    "import time\n",
    "import warnings\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "# Setup Selenium browser\n",
    "CHROME_DRIVER_LOCATION = \"chromedriver.exe\"\n",
    "OPTIONS = webdriver.ChromeOptions()\n",
    "OPTIONS.add_argument('--ignore-certicate-errors')\n",
    "OPTIONS.add_argument('--incognito')\n",
    "#OPTIONS.add_argument('--headless')\n",
    "OPTIONS.add_experimental_option('excludeSwitches', ['enable-logging'])\n",
    "driver = webdriver.Chrome(executable_path=CHROME_DRIVER_LOCATION,options=OPTIONS)\n",
    "#driver.implicitly_wait(10)\n",
    "wait = WebDriverWait(driver, 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to open the TSX listings page, select a letter and exchange to extract data from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to open TSX Page for symbols listed, select the exchange and grab data from page\n",
    "# Complicated scraper as data is JAVASCRIPT generated, cannot simply use pandas_datareader\n",
    "# Returns a Pandas DataFrame with scrapped data\n",
    "def tickers_currenlty_listed(letter, exchange):\n",
    "    tsx_url = f\"https://www.tsx.com/listings/listing-with-us/listed-company-directory\"\n",
    "    driver.get(tsx_url)\n",
    "    \n",
    "    # Set exchange for page\n",
    "    try:\n",
    "        btn_switch = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.ID, 'exchange-toggle')))\n",
    "        if exchange == \"tsx\" and btn_switch.get_attribute(\"class\") == \"btn-switch invert\":\n",
    "            btn_switch.click()\n",
    "        if exchange == \"tsxv\" and btn_switch.get_attribute(\"class\") == \"btn-switch\":\n",
    "            btn_switch.click()\n",
    "    except Exception as e:\n",
    "        print(f\"Unable to locate exchange-toggle button, \",e)\n",
    "        return {}\n",
    "\n",
    "    # Push letter+ENTER in search field\n",
    "    try:\n",
    "        search = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.ID, 'query')))\n",
    "        search.send_keys(letter + Keys.ENTER)\n",
    "    except Exception as e:\n",
    "        print(f\"Unable to send_keys in inut field [{letter} ], \",e)\n",
    "        return {}    \n",
    "\n",
    "    # Extract data using selenium ad create a list\n",
    "    try:\n",
    "        #WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.XPATH, \"//*[text()='Symbol']\")))\n",
    "        result_data_xpath = '//*[@id=\"tresults\"]/tbody'\n",
    "        WebDriverWait(driver, 5).until( EC.visibility_of_element_located((By.XPATH, result_data_xpath)))\n",
    "        datagrid = driver.find_element_by_xpath(result_data_xpath)\n",
    "    except Exception as e:\n",
    "        print(f\"Unable to locate HTML Table containing listing data, \",e)\n",
    "        return {}    \n",
    "\n",
    "    data_rows = datagrid.find_elements(By.TAG_NAME, \"tr\")\n",
    "    data = []\n",
    "    for row in data_rows:\n",
    "        cells = row.find_elements(By.TAG_NAME, \"td\")\n",
    "        # Skip the line with no ticker symbol\n",
    "        if (cells[1].text != \"\"):\n",
    "            ticker = cells[1].text\n",
    "            name = cells[0].text\n",
    "            url = f\"https://money.tmx.com/en/quote/{ticker}\"\n",
    "            row_info = {\"ticker\": ticker, \"company\":name.strip(), \"exchange\": exchange, \"url\":url, \"yahoo\":\"-\"}\n",
    "            data.append(row_info)\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to open the TSX Trade history page for a specified TICKER symbol and extract data to a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_trading_history(symbol, pages_to_read, msg1):\n",
    "# Function to extract trading history for tmx web site and store in a CSV file\n",
    "# Parameters:\n",
    "# - symbol to extract\n",
    "# - pages_to_read : number of historical pages to read (selenium script will click on the navigation buttons automatically)\n",
    "# - msg1 : Use when looping through a bunch of tickers to follow progress\n",
    "\n",
    "    tmx_url = f\"https://money.tmx.com/en/quote/{symbol}/trade-history?selectedTab=price-history\"\n",
    "    driver.get(tmx_url)\n",
    "    header_flag = True\n",
    "    #username = input(\"Please click in web page then ENTER\")\n",
    "    time.sleep(5)\n",
    "\n",
    "    # CLOSE THE Freaking AD at bottom of screen to expose the next button\n",
    "    ad_closed = False\n",
    "    try:\n",
    "        #close_ad = WebDriverWait(driver, 5).until(presence_of_element_located(By.ID('ssrt-close-anchor-button')))\n",
    "        close_ad = WebDriverWait(driver, 5).until(EC.presence_of_element_located((By.ID, 'ssrt-close-anchor-button')))\n",
    "        print(type(close_ad))\n",
    "        close_ad.click()\n",
    "        ad_closed = True\n",
    "    except Exception as e:\n",
    "        print(f\"Unable to clase_ads : {e}\")   \n",
    "\n",
    "    if ad_closed:\n",
    "        # Set to pages_to_read to 45 for a good 2 years back data ( for does not loop through last number of range)\n",
    "        fin_loop = pages_to_read + 1\n",
    "        prev_date = \"None\"\n",
    "        for i in range(1,fin_loop):\n",
    "            html_page = driver.page_source\n",
    "            try:\n",
    "                data = pd.read_html(html_page)\n",
    "                prices_df = data[0]\n",
    "                #print(f\"Data extracted: {i}, date at bottom : {prices_df['Date'].iloc[-1]}\")\n",
    "                print(f\"{msg1} => {symbol} [{i}/{fin_loop - 1}] :Extracted date: {prices_df['Date'].iloc[-1]}, Previous Date Extracted: {prev_date}\")\n",
    "                filename = f\"CSV\\{symbol}.csv\"\n",
    "                prices_df.to_csv(filename, mode='a', header=header_flag)\n",
    "                header_flag = False\n",
    "                prev_date = prices_df['Date'].iloc[-1]        \n",
    "                # Find the next button and click it\n",
    "                btn_next = driver.find_element(By.XPATH, \"//button[@data-testid='next-button']\")\n",
    "                btn_next.click() \n",
    "            except:\n",
    "                i = fin_loop\n",
    "    \n",
    "    print(\"End of extraction\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to load a CSV file to a pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_prices_from_csv(path, file, symbol_to_load):\n",
    "    file_to_load = path + file\n",
    "    try:\n",
    "        data = pd.read_csv(f\"{file_to_load}\")\n",
    "        # Clean the data make sure all new data rows are unique\n",
    "        if data.Date.is_unique == False:\n",
    "            data_cleaned = data[~data.Date.duplicated()]\n",
    "        else:\n",
    "            data_cleaned = data\n",
    "    except Exception as e:\n",
    "        print(f\"Error opening file : {e}\")\n",
    "        return None\n",
    "\n",
    "    # # Structure the data according to database table structure\n",
    "    # data_cleaned.drop(['Unnamed: 0', 'VWAP ($)', 'Change ($)', 'Trade Value', '# Trades', 'Change (%)'], axis=1, inplace=True)\n",
    "    # data_cleaned.rename(columns={'Open ($)': 'Open', 'High ($)': 'High', 'Low ($)': 'Low', 'Close ($)': 'Close'  }, inplace=True)\n",
    "    # data_cleaned[\"Ticker\"] = symbol_to_load\n",
    "    # data_cleaned['Date'] = pd.to_datetime(data_cleaned[\"Date\"], infer_datetime_format=True)\n",
    "    # data_cleaned = data_cleaned.reindex([\"Date\",\"Ticker\",\"Open\",\"High\",\"Low\",\"Close\",\"Volume\"],axis=1)\n",
    "\n",
    "    return data_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to save a DataFrame (loaded from CSV file) to database table : prices_daily  \n",
    "Date from CSV must be formatted to yyyy-mm-dd  \n",
    "Data will be appended only for dates greater than existing Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "def update_prices_daily(ticker, price_data):\n",
    "        conn = sqlite3.connect(\"TSX_Prices.sqlite\")\n",
    "        if conn is not None:\n",
    "            symbol_to_load = ticker\n",
    "            existing_prices_df = pd.read_sql(f\"SELECT * FROM 'prices_daily' WHERE Ticker='{symbol_to_load}' ORDER BY Date DESC LIMIT 1\", conn)\n",
    "        try:\n",
    "            # Only keep dataframe dates that are greater than the existing ones in prices_daily\n",
    "            last_date = existing_prices_df.loc[0][\"Date\"]\n",
    "            new_prices_df = price_data.loc[price_data[\"Date\"] > last_date]\n",
    "        except KeyError:\n",
    "            last_date = None\n",
    "            new_prices_df = price_data\n",
    "\n",
    "        # Structure the data according to database table structure\n",
    "        new_prices_df.drop(['VWAP ($)', 'Change ($)', 'Trade Value', '# Trades', 'Change (%)'], axis=1, inplace=True)\n",
    "        new_prices_df.rename(columns={'Open ($)': 'Open', 'High ($)': 'High', 'Low ($)': 'Low', 'Close ($)': 'Close'  }, inplace=True)\n",
    "        new_prices_df[\"Ticker\"] = symbol_to_load\n",
    "        #new_prices_df['Date'] = pd.to_datetime(data_cleaned[\"Date\"], infer_datetime_format=True)\n",
    "        new_prices_df = new_prices_df.reindex([\"Date\",\"Ticker\",\"Open\",\"High\",\"Low\",\"Close\",\"Volume\"],axis=1)\n",
    "        # Drop duplicate dates\n",
    "        new_prices_df.drop_duplicates(subset=\"Date\", inplace=True)\n",
    "        new_prices_df.to_sql(\"prices_daily\", conn, if_exists=\"append\")\n",
    "\n",
    "        print(f\"\\n\\nUpdating 'prices_daily': {ticker} \\n{price_data} \\nExisting Prices\\n{existing_prices_df} \\nLast date: {last_date} \\nNew Prices:\\n{new_prices_df}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manual Extraction functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List symbols with no price data for a specific exchange (tsx or tsxv)\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "exchange = \"tsxv\"\n",
    "conn = sqlite3.connect(\"TSX_Prices.sqlite\")\n",
    "sql = f\"SELECT ticker FROM 'symbols' WHERE exchange = '{exchange}' AND ticker NOT IN (  SELECT DISTINCT Ticker FROM 'prices_daily' )\"\n",
    "data = pd.read_sql(sql, conn)\n",
    "symbols_list = data[\"ticker\"].to_list()\n",
    "print(symbols_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract trading history and store to csv file (in CSV folder)\n",
    "for symbol in symbols_list:\n",
    "    extract_trading_history(symbol, 1, symbol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "path = \"CSV\\TSXV\\\\\"\n",
    "files = [x for x in listdir(path) if isfile(join(path,x)) ]\n",
    "tickers = [os.path.splitext(x)[0] for x in files]\n",
    "\n",
    "for file in files:\n",
    "    ticker,_ = os.path.splitext(file)\n",
    "    data = load_prices_from_csv(path, file, ticker)\n",
    "    update_prices_daily(ticker, data)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d67b9618866dd9b74d33a444efc9c9bbb974433ef649364e533196a1f341c63f"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
