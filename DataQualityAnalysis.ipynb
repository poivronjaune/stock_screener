{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DATA MANAGEMENT\n",
    "Data Quality Analysis and Database Control \n",
    "All features are wrapped in python functions (use Run All to define all functions and launch them individually)  \n",
    "Make sure to comment out the functions call to prevent lauching destructive features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import pandas_datareader as pdr\n",
    "import numpy as np\n",
    "import datetime\n",
    "import plotly.graph_objects as go\n",
    "import talib as ta \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DATA ACCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to return a Dataframe with all symbols available in database\n",
    "def get_all_symbols():\n",
    "    \"\"\" Get all symbols in database as a DataFrame \"\"\"\n",
    "    conn = sqlite3.connect(\"TSX_Quality.sqlite\")\n",
    "    sql = f\"SELECT * FROM symbols ORDER BY UPPER(ticker) ASC\"\n",
    "    data = pd.read_sql_query(sql, conn, index_col=\"ticker\")\n",
    "    #data.drop(labels=[\"index\", \"url\", \"yahoo\"], axis=1, inplace=True )\n",
    "\n",
    "    return data\n",
    "\n",
    "symbols_df = get_all_symbols()\n",
    "symbols_df.isna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heavy function to retreive all prices from database (more than 5 million rows, takes many seconds to execute)\n",
    "def get_all_prices():\n",
    "    conn = sqlite3.connect(\"TSX_Quality.sqlite\")\n",
    "    sql = f\"SELECT * FROM prices_daily ORDER BY UPPER(Ticker) ASC, Date ASC\"\n",
    "    prices = pd.read_sql_query(sql, conn, index_col=\"Date\")\n",
    "    prices.index = pd.to_datetime(prices.index)\n",
    "    #prices = pd.read_sql_query(sql, conn, index_col=\"ticker\")\n",
    "    #prices.drop(\"index\", axis=1, inplace=True)\n",
    "    \n",
    "    return prices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CREATE DB FROM CSV Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a local SQLITE database with over 6 million lines of price data\n",
    "# Unzip the price data zipped files to get the CSV  \n",
    "# Launch the create_database() function with the CSV file you want to load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete existing database file\n",
    "# RUN symbols first to overwrite database if exists, prices will append.\n",
    "def create_symbols_table(db, csv_file):\n",
    "    data = pd.read_csv(f\"{csv_file}\", index_col=\"ticker\")\n",
    "\n",
    "    conn = sqlite3.connect(db)\n",
    "    data.to_sql(\"Symbols\", conn, if_exists='replace', index=True)\n",
    "    conn.close()\n",
    "    return data   \n",
    "\n",
    "def create_prices_history(db, csv_file):\n",
    "    data = pd.read_csv(f\"{csv_file}\", index_col=\"Date\", dtype={'Open': str, 'High': str, 'Low': str, 'Close': str, 'Volume': str})\n",
    "    data.index = pd.to_datetime(data.index)\n",
    "    data = data[[\"Ticker\",\"Open\",\"High\",\"Low\",\"Close\",\"Volume\"]]\n",
    "    conn = sqlite3.connect(db)\n",
    "    data.to_sql(\"Prices_Daily\", conn, if_exists='append', index=True)\n",
    "    conn.close()\n",
    "    # All columns other than index (Date) are string values that can contain \"-\" characters be careful\n",
    "    return data\n",
    "\n",
    "# db_name = \"TEST1.sqlite\"\n",
    "# symbols_file = \"symbols.csv\"\n",
    "# create_symbols_table(db_name, symbols_file)\n",
    "\n",
    "# prices_file = \"prices_2022.csv\"\n",
    "# create_prices_history(db_name, prices_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### YAHOO DATA EXTRACTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN this feature only once if you do NOT have historical data  \n",
    "# All price data will be stored in a CSV file under (CSV folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get historical data from Yahoo finance and store as a CSV file\n",
    "# Also cumulates symbols that are not available on Yahoo Finance for further investigation\n",
    "# TODO: Modify this function to pass start and end date for extraction if data before 2014 is required\n",
    "def yahoo_to_csv(ticker, exchange):\n",
    "    #start_date = \"2014-01-01\"\n",
    "    start_date = \"2012-01-01\"\n",
    "    end_date   = str(datetime.datetime.now().strftime(\"%Y-%m-%d\"))\n",
    "    if exchange == \"tsx\":\n",
    "        yahoo_symbol = ticker.replace(\".\", \"-\") + \".TO\"\n",
    "    else:\n",
    "        yahoo_symbol = ticker.replace(\".\", \"-\") + \".V\"\n",
    "\n",
    "    try:\n",
    "        data = pdr.DataReader(yahoo_symbol, \"yahoo\", start_date, end_date)\n",
    "        data[\"Ticker\"] = ticker\n",
    "        data.index = pd.to_datetime(data.index)\n",
    "        data.to_csv(f\"CSV/{ticker}.csv\", index_label=\"Date\", mode=\"w\", date_format=\"%Y-%m-%d %H:%M:%S\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Unable to read Data from Yahoo : {e}\")\n",
    "        return None\n",
    "\n",
    "# yahoo_to_csv(\"SHOP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell to loop through all symbols and extract price data from yahoo\n",
    "def loop_through_symbols_on_yahoo():\n",
    "    not_found_on_yahoo = []\n",
    "    df = get_all_symbols()\n",
    "    df[\"YahooExists\"] = False\n",
    "    #df_t = df[40:70]\n",
    "    df_t = df\n",
    "\n",
    "    for index, row in df_t.iterrows():\n",
    "        symbol = index\n",
    "        exchange = row[\"exchange\"]\n",
    "        result = yahoo_to_csv(symbol, exchange)\n",
    "        if result is not None:\n",
    "            df.at[symbol, \"YahooExists\"] = True\n",
    "        else:\n",
    "            not_found_on_yahoo.append(symbol)\n",
    "\n",
    "    print(not_found_on_yahoo)\n",
    "    notfound_df = pd.DataFrame(not_found_on_yahoo)\n",
    "    notfound_df.to_csv(\"notfoundonyahoo.csv\", mode=\"w\", index=False, header=False )\n",
    "# 3634 elements from yahoo in 1:11 hours\n",
    "\n",
    "#loop_through_symbols_on_yahoo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TSX SCRAPER  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trade History Extractor (Scrapes TSX Web site)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Web scraper for TSX (when we can't get it from yahoo finance)\n",
    "import time\n",
    "import random\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selenium driver to simulate humain interaction with the web site\n",
    "def open_browser():\n",
    "    # Setup Selenium browser\n",
    "    CHROME_DRIVER_LOCATION = \"chromedriver.exe\"\n",
    "    service_object = Service(CHROME_DRIVER_LOCATION)\n",
    "\n",
    "    OPTIONS = webdriver.ChromeOptions()\n",
    "    OPTIONS.add_argument('--ignore-certicate-errors')\n",
    "    OPTIONS.add_argument('--incognito')\n",
    "    #OPTIONS.add_argument('--headless')\n",
    "    OPTIONS.add_experimental_option('excludeSwitches', ['enable-logging'])\n",
    "    \n",
    "    #self.driver = webdriver.Chrome(executable_path=CHROME_DRIVER_LOCATION,options=OPTIONS)\n",
    "    driver = webdriver.Chrome(service=service_object, options=OPTIONS)\n",
    "    WebDriverWait(driver, 10)\n",
    "\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Special feature to click on the close button for ads presented on the TSX web site\n",
    "def close_add(driver):\n",
    "    try:\n",
    "        close_ad_btn = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.ID, 'ssrt-close-anchor-button')))\n",
    "    except Exception as e:\n",
    "        print(f\"Unable to close_ads \")  \n",
    "        return False \n",
    "\n",
    "    try:\n",
    "        close_ad_btn.click()\n",
    "        return True\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature to click the next button on the Trade Histiry page of TSX\n",
    "# This change the selenium controlled web browser page\n",
    "def next_page(driver):\n",
    "    # btn_next = driver.find_element(By.XPATH, \"//button[@data-testid='next-button']\")\n",
    "    try:\n",
    "        next_btn = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, \"//button[@data-testid='next-button']\")))\n",
    "        next_btn.click()\n",
    "        #print(f\"Next button element found on page...\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Next button element NOT FOUND :\")  \n",
    "        return False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrap TSX prices from the current Selenium controlled web Browser\n",
    "def tsx_to_csv(driver, ticker, num_pages):\n",
    "    url = f\"https://money.tmx.com/en/quote/{ticker}/trade-history?selectedTab=price-history\"\n",
    "    driver.get(url)\n",
    "    random_delay = random.randint(2, 6)\n",
    "    #print(f\"Random sleep delay : {random_delay}\")\n",
    "    time.sleep(random_delay)\n",
    "    ad_closed = close_add(driver)\n",
    "\n",
    "    column_names = [\"Date\", \"Open ($)\", \"High ($)\", \"Low ($)\", \"Close ($)\", \"VWAP ($)\", \"Change ($)\", \"Change (%)\", \"Volume\", \"Trade Value\", \"# Trades\"]\n",
    "    data_df = pd.DataFrame(columns=column_names)\n",
    "\n",
    "    if ad_closed:\n",
    "        # Loop through pages\n",
    "        for i in range(0, num_pages):\n",
    "            html_page = driver.page_source \n",
    "            try:\n",
    "                html_data = pd.read_html(html_page)\n",
    "            except:\n",
    "                html_data = []\n",
    "            \n",
    "            if len(html_data) == 0:\n",
    "                i = num_pages\n",
    "                prices_df = None\n",
    "            else:\n",
    "                prices_df = html_data[0]\n",
    "                if prices_df.empty:\n",
    "                    i = num_pages\n",
    "                else:\n",
    "                    data_df = data_df.append(prices_df)\n",
    "                    prices_df = None\n",
    "                    # click next page button\n",
    "                    next_page(driver)\n",
    "                    random_delay = random.randint(2, 6)\n",
    "                    #print(f\"Random sleep delay NEXT PAGE : {random_delay}\")\n",
    "                    time.sleep(random_delay)\n",
    "\n",
    "        if not data_df.empty:\n",
    "            data_df.index = pd.to_datetime(data_df[\"Date\"], infer_datetime_format=True)\n",
    "            data_df.drop(columns=[\"Date\",\"VWAP ($)\",\"Change ($)\",\"Change (%)\",\"Trade Value\",\"# Trades\"], inplace=True)\n",
    "            data_df = data_df.rename(columns={'Open ($)': 'Open', 'High ($)': 'High', 'Low ($)': 'Low', 'Close ($)': 'Close'})\n",
    "            data_df[\"Adj Close\"] = -1\n",
    "            data_df[\"Ticker\"] = ticker\n",
    "            data_df.to_csv(f\"TSXCSV/{ticker}.csv\", index_label=\"Date\", mode=\"w\", date_format=\"%Y-%m-%d %H:%M:%S\")\n",
    "            return data_df\n",
    "        else:\n",
    "            print(f\"No data scrapped - no prices found, {ticker}\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"No data scrapped - problem with ads, {ticker}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If new symbols that do not exist on Yahoo API, loop through them to scrape history prices data from TSX\n",
    "# If you want to scrape the latest page of prices data, change [ tsx_to_csv(driver, symbol, 100) ] to [ tsx_to_csv(driver, symbol, 1) ]\n",
    "def loop_through_missing_symbols(loops, pages):\n",
    "    notfound_symbols = pd.read_csv(\"notfound.csv\", header=None)\n",
    "    symbols_list = notfound_symbols[0].tolist()\n",
    "    start_index = 0\n",
    "    step = loops\n",
    "    driver = open_browser()\n",
    "    for symbol in symbols_list[start_index:start_index+step]:\n",
    "        p1 = tsx_to_csv(driver, symbol, pages)\n",
    "        print(f\"Symbol : {symbol}\")\n",
    "\n",
    "# (number of symbols to extract, number of pages from trade history to grab)  \n",
    "# Remove symbol PRN as it is a reserved windows command for files  \n",
    "\n",
    "#loop_through_missing_symbols(80, 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DATA EXPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export Our Database Price History to a CSV File\n",
    "\n",
    "# Export SQLite3 Prices to CSV file for github push using yearly export\n",
    "# Needs to be Zipped to stay within github limits\n",
    "def yearly_prices_to_csv(year):\n",
    "    conn = sqlite3.connect(\"TSX_Quality.sqlite\")\n",
    "    sql = f\"SELECT * FROM 'prices_daily' WHERE Date LIKE '{year}%' ORDER BY ticker ASC, Date DESC\"\n",
    "    data = pd.read_sql_query(sql, conn)\n",
    "    #data.drop(labels=\"index\", axis=1, inplace=True)\n",
    "    #data[\"Date\"] = pd.to_datetime(data[\"Date\"], infer_datetime_format=True)\n",
    "    #data[\"Date\"] = data[\"Date\"].dt.date\n",
    "    data.to_csv(f\"prices_{year}.csv\", index=False)\n",
    "\n",
    "#yearly_prices_to_csv(\"2014\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DATA UPDATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through CSV folder to insert new data obtained from TSX Scraper or Yahoo\n",
    "\n",
    "def csv_to_dataframe(folder, file):\n",
    "    data = pd.read_csv(f\"{folder}/{file}\", index_col=\"Date\")\n",
    "    data.index = pd.to_datetime(data.index)\n",
    "    data = data[[\"Ticker\",\"Open\",\"High\",\"Low\",\"Close\",\"Volume\"]]\n",
    "    return data\n",
    "\n",
    "def insert_new_prices_in_DB(new_prices, db_prices, conn):\n",
    "    ticker = new_prices[\"Ticker\"].values[0]\n",
    "    existing_prices = db_prices.loc[db_prices[\"Ticker\"] == ticker]\n",
    "    filter = new_prices.index.isin(existing_prices.index)\n",
    "    new_prices.drop(new_prices[filter].index, inplace = True)\n",
    "    #conn = sqlite3.connect(\"TSX_Quality.sqlite\")    \n",
    "    new_prices.to_sql(\"Prices_Daily\", conn, if_exists='append', index=True)\n",
    "    return new_prices\n",
    "\n",
    "# LOOP THROUGH CSV Folders and insert new data into the Prices_Daily table\n",
    "def loop_through_local_csv(existing_prices, conn, folder):\n",
    "    csv_files = os.listdir(f\"{folder}\")\n",
    "    for file in csv_files:\n",
    "        new_data = csv_to_dataframe(folder, file)\n",
    "        insert_new_prices_in_DB(new_data, existing_prices, conn)\n",
    "        print(f\"Finished processing : {file}\")\n",
    "\n",
    "\n",
    "# conn1 = sqlite3.connect(\"TSX_Quality.sqlite\")\n",
    "# db_prices = get_all_prices()\n",
    "# new_data = csv_to_dataframe(f\"TSXCSV\", \"SLF.PR.H.csv\")\n",
    "# insert_new_prices_in_DB(new_data, db_prices, conn1)\n",
    "\n",
    "# conn1 = sqlite3.connect(\"TSX_Quality.sqlite\")\n",
    "# db_prices = get_all_prices()\n",
    "# loop_through_local_csv(db_prices, conn1, \"TSXCSV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DATA QUALITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the first price data for every ticker (to help in finding previous date for GAP analysis)\n",
    "# Prices must be sorted by ascending  ticker symbol and ascending dates\n",
    "# For every ticker+date combination, insert the date of the previous price data fo rthe same ticker (to calculate the number of days between data and detect missing prices)\n",
    "def detect_missing_prices(prices):\n",
    "    prices[\"new_ticker\"] = np.where(prices[\"Ticker\"] != prices[\"Ticker\"].shift(1), \"New\", \"\")\n",
    "    prices[\"cur_date\"]  = pd.to_datetime(prices.index, format=\"%Y-%m-%d\", errors='coerce')\n",
    "    prices[\"prev_date\"] = pd.to_datetime(np.where(prices[\"new_ticker\"] != \"New\", prices[\"cur_date\"].shift(1), None), format=\"%Y-%m-%d\", errors='coerce')\n",
    "\n",
    "    # Calculate date gaps in prices using succesive dates for tickers in database\n",
    "    prices[\"GAP\"] = prices[\"cur_date\"] - prices[\"prev_date\"]\n",
    "    prices[\"missing\"] = prices[\"GAP\"] > datetime.timedelta(days=5)\n",
    "\n",
    "    \n",
    "# Show date GAPS for a specific symbol\n",
    "def show_missing_prices(prices, ticker=None):\n",
    "    if ticker is None:\n",
    "        filter = (prices[\"missing\"] == True)\n",
    "    else:\n",
    "        filter = (prices[\"missing\"] == True) & (prices[\"Ticker\"] == ticker)\n",
    "    \n",
    "    missing_data = prices.loc[filter]\n",
    "    return missing_data\n",
    "\n",
    "# prices = get_all_prices()\n",
    "# detect_missing_prices(prices)\n",
    "# missing_df = show_missing_prices(prices)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f6ea743d4123cf0f72c1a3e7afb768a545bcfd9cdf4233ff0f334085311ff027"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
